Docker Threat Modeling
Given Docker's system components and the STRIDE framework, identify five potential threats that may arise.

1)
[FAIL] 5.10 Ensure that the memory usage for container is limited (Scored)
You can use memory limit
mechanisms to prevent a denial of service occurring where one container consumes all of
the hostâ€™s resources and other containers on the same host are therefore not able to
function. Having no limit on memory usage can lead to issues where one container can
easily make the whole system unstable and as a result unusable.

2)
[FAIL] 2.2.b Ensure the logging level is set to 'info' (Scored)
Setting up an appropriate log level, configures the Docker daemon to log events that you
would want to review later. A base log level of info and above would capture all logs

3)
[FAIL] 3.17 Ensure that the daemon.json file ownership is set to root:root
The daemon.json file contains sensitive parameters that could alter the behavior of the
docker daemon. It should therefore be owned and group owned by root to ensure it can
not be modified by less privileged users.

4)
3.18 Ensure that daemon.json file permissions are set to 644 or more
restrictive (Scored)
The daemon.json file contains sensitive parameters that may alter the behavior of the
docker daemon. Therefore it should be writeable only by root to ensure it is not modified
by less privileged users.

5)
[FAIL] 2.14.a Ensure Userland Proxy is Disabled (Scored)
The Docker engine provides two mechanisms for forwarding ports from the host to
containers, hairpin NAT, and the use of a userland proxy. In most circumstances, the
hairpin NAT mode is preferred as it improves performance and makes use of native Linux
iptables functionality instead of using an additional component.
Where hairpin NAT is available, the userland proxy should be disabled on startup to reduce
the attack surface of the installation.


Kubernetes Threat Modeling
Given Kubernetes' system components and the STRIDE framework, identify five potential threats that may arise.
https://ranchermanager.docs.rancher.com/v2.6/reference-guides/rancher-security/rancher-v2.6-hardening-guides/rke2-hardening-guide-with-cis-v1.6-benchmark
1)
[FAIL] 1.1.12 Ensure that the etcd data directory ownership is set to etcd:etcd
etcd is a highly-available key-value store used by Kubernetes deployments for persistent
storage of all of its REST API objects. This data directory should be protected from any
unauthorized reads or writes. It should be owned by etcd:etcd.

2)
[FAIL] 1.2.6 Ensure that the --kubelet-certificate-authority argument is set as appropriate (Automated)
Kubernetes API server talks to the kubelet on each node. If you do not set certificate authority TLS cert from any kubelet maybe accepted.
This allows man in the middle attacks


3)
[FAIL] 1.2.10 Ensure that the admission control plugin EventRateLimit is set (Automated)
EventRateLimit controls how many Kubernetes events (like pod create, delete, etc.) can be generated over time.
It protects against event spam.


4)
[FAIL] 1.2.17 Ensure that the admission control plugin NodeRestriction is set (Automated)
NodeRestriction prevents kubelets from performing unsafe or unauthorized actions via the API on other nodes.
Portects against: Node spoofing, pod tampering, privilege escalation


Docker-bench Run Results and Analysis
From the failed findings, select and document 3 findings from the Docker-bench results that you want to harden based on the 5 attack surface areas you identified in Step 1. At least 1 of the 3 hardened findings should be different from the ones mentioned in the exercise (i.e. 5.10, 5.14, and 5.22).
#Fixes applied:
1)
[FAIL] 1.1.12 Ensure that the etcd data directory ownership is set to etcd:etcd
groupadd --gid 52034 etcd
useradd --comment "etcd service account" --uid 52034 --gid 52034 etcd
chown etcd:etcd /var/lib/etcd
docker run --pid=host -v /etc/passwd:/etc/passwd -v /etc/group:/etc/group -v /etc:/node/etc:ro -v /var:/node/var:ro -ti rancher/security-scan:v0.2.2 bash
kube-bench run --targets etcd --scored --config-dir=/etc/kube-bench/cfg --benchmark rke-cis-1.6-hardened | grep FAIL

2)
[FAIL] 1.2.6 Ensure that the --kubelet-certificate-authority argument is set as appropriate (Automated)
Adapt cluster.yml:
kube-api:
    image: ""
    extra_args: #{}
      kubelet-certificate-authority: "/etc/kubernetes/ssl/kube-ca.pem"

3)
[FAIL] 1.2.10 Ensure that the admission control plugin EventRateLimit is set (Automated)
Adapt cluster.yml:
services:
  kube-api:
    extra_args:
      admission-control-config-file: "/etc/kubernetes/admission-config.yaml"

sudo vim /etc/kubernetes/admission-config.yaml
Edit/copy config

4)
[FAIL] 1.2.17 Ensure that the admission control plugin NodeRestriction is set (Automated)
Adapt cluster.yml:
services:
  kube-api:
    extra_args:
      enable-admission-plugins: "...,NodeRestriction"

#Reload Cluster.yaml
rke up --ignore-docker-version

# Final test on node:
docker run --pid=host -v /etc/passwd:/etc/passwd -v /etc/group:/etc/group -v /etc:/node/etc:ro -v /var:/node/var:ro -ti rancher/security-scan:v0.2.2 bash
kube-bench run --targets etcd,master,controlplane,policies --scored --config-dir=/etc/kube-bench/cfg --benchmark rke-cis-1.6-hardened | grep FAIL


?
ps -ef | grep etcd
/usr/local/bin/etcd
stat -c %U:%G /usr/local/bin/etcd

2)
Set kernel parameters
cd /etc/sysctl.d
vim 90-kubelet.conf 
    Config:
        vm.overcommit_memory=1
        vm.panic_on_oom=0
        kernel.panic=10
        kernel.panic_on_oops=1
        kernel.keys.root_maxbytes=25000000
        Apply: sysctl -p /etc/sysctl.d/90-kubelet.conf
Run Container: docker run --pid=host -v /etc/passwd:/etc/passwd -v /etc/group:/etc/group -v /etc:/node/etc:ro -v /var:/node/var:ro -ti rancher/security-scan:v0.2.2 bash
Run Test: kube-bench run --targets etcd --scored --config-dir=/etc/kube-bench/cfg --benchmark rke-cis-1.6-hardened | grep FAIL

3) Nicht angewendet aber 3.1
touch account_update.yaml
vim account_update.yaml
    Content: 
        apiVersion: v1
        kind: ServiceAccount
        metadata:
        name: default
        automountServiceAccountToken: false
touch account_update.sh
    Content:
        #!/bin/bash -e
        for namespace in $(kubectl get namespaces -A -o=jsonpath="{.items[*]['metadata.name']}"); do
            echo -n "Patching namespace $namespace - "
        kubectl patch serviceaccount default -n ${namespace} -p "$(cat account_update.yaml)"
        done
sudo chmod +x account_update.sh    


3.1) 
Control 5.1.5 Ensure that default service accounts are not actively used
kubectl --kubeconfig kube_config_cluster.yml get namespaces -A -o=jsonpath="{.items[*]['metadata.name']}"
We have this namespaces:
default 
ingress-nginx 
kube-node-lease 
kube-public 
kube-system
We will not touch the "kube.." namespaces but fix automountServiceAccountToken for the others:
a) kubectl --kubeconfig kube_config_cluster.yml patch serviceaccount default -n default -p '{"automountServiceAccountToken": false}'
b) kubectl --kubeconfig kube_config_cluster.yml patch serviceaccount default -n ingress-nginx -p '{"automountServiceAccountToken": false}'
REstart deployments: 
- kubectl --kubeconfig kube_config_cluster.yml rollout restart deployment -n default
kubectl --kubeconfig kube_config_cluster.yml rollout restart deployment -n ingress-nginx


5.2.2 Minimize the admission of containers wishing to share the host
process ID namespace (Manual)
A container running in the host's PID namespace can inspect processes running outside the
container. If the container also has access to ptrace capabilities this can be used to escalate
privileges outside of the container.
There should be at least one PodSecurityPolicy (PSP) defined which does not permit
containers to share the host PID namespace.



-- tbd

Control 5.3.2
Ensure that all Namespaces have Network Policies defined
Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. 
Network segmentation is important to ensure that containers can communicate only with those they are supposed to. 
A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints.







